{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-03T20:09:47.932791500Z",
     "start_time": "2023-12-03T20:09:47.899792600Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import svm\n",
    "from typing import Dict\n",
    "from sklearn import metrics\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import glob\n",
    "import torch\n",
    "import wget\n",
    "import zipfile\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from gensim.models.fasttext import FastText\n",
    "\n",
    "from mittens import GloVe\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import load_model\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "from setfit import SetFitModel, SetFitTrainer\n",
    "from datasets import load_dataset, logging\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import XLNetForSequenceClassification, RobertaForSequenceClassification\n",
    "from transformers import XLMRobertaForSequenceClassification, DistilBertForSequenceClassification\n",
    "from transformers import RobertaTokenizer, XLMRobertaTokenizer, DistilBertTokenizer, XLNetTokenizer"
   ],
   "id": "df221779b7d08694"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common Helper Functions"
   ],
   "id": "4776c40ef8656368"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-03T20:09:48.976654600Z",
     "start_time": "2023-12-03T20:09:48.919654100Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_avg_report(results, folds):\n",
    "    \n",
    "    \"\"\"\n",
    "    function takes the input of predicted model results on five folds and returns\n",
    "    average of weighted and macro Precision, Recall, F-1 \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    weighted_precision = []\n",
    "    weighted_recall = []\n",
    "    weighted_f1 = []\n",
    "    \n",
    "    macro_precision = []\n",
    "    macro_recall = []\n",
    "    macro_f1 = []\n",
    "    \n",
    "    for result_df in results:                        \n",
    "        res_rows = result_df.tail(3)\n",
    "\n",
    "        precision_scores =  res_rows['precision'].tolist()\n",
    "        recall_scores =  res_rows['recall'].tolist()\n",
    "        f1_scores =  res_rows['f1-score'].tolist()\n",
    "\n",
    "        precision_macro_avg =  precision_scores[1]\n",
    "        precision_weighted_avg = precision_scores[2]\n",
    "\n",
    "        recall_macro_avg =  recall_scores[1]\n",
    "        recall_weighted_avg = recall_scores[2]\n",
    "\n",
    "        fl_accuracy = f1_scores[0]\n",
    "        f1_scores_macro_avg =  f1_scores[1]\n",
    "        f1_scores_weighted_avg = f1_scores[2]\n",
    "                \n",
    "        weighted_precision.append(precision_weighted_avg)\n",
    "        weighted_recall.append(recall_weighted_avg)\n",
    "        weighted_f1.append(f1_scores_weighted_avg)\n",
    "        \n",
    "        macro_precision.append(precision_macro_avg)\n",
    "        macro_recall.append(recall_macro_avg)\n",
    "        macro_f1.append(f1_scores_macro_avg)\n",
    "                \n",
    "    weighted_average = round(sum(weighted_precision) / folds, 2), round(sum(weighted_recall) / folds, 2), round(sum(weighted_f1) / folds, 2)\n",
    "    macro_average = round(sum(macro_precision) / folds, 2), round(sum(macro_recall) / folds, 2), round(sum(macro_f1) / folds, 2)\n",
    "            \n",
    "    return weighted_average, macro_average\n",
    "\n",
    "def get_accuracy(y_actual, y_predicted):\n",
    "    \"\"\"\n",
    "    function takes the actual and predicted labels to return\n",
    "    the accuracy per fold\n",
    "    \n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    for index in zip(y_actual, y_predicted):\n",
    "        \n",
    "        if index[0] == index[1]:\n",
    "                count += 1\n",
    "    topk_acc = round(count / len(y_actual), 2)\n",
    "    return topk_acc\n"
   ],
   "id": "ac315cd0580892a1"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML alogrithms Pipeline"
   ],
   "id": "4ff7067fa8801719"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-03T20:11:13.004898700Z",
     "start_time": "2023-12-03T20:11:12.960893Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_ML_model_files(model_name, model_path, pca):\n",
    "    \n",
    "    \"\"\"\n",
    "    function load the ML models relevant files based \n",
    "    on the parameters given\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    ML_model = pickle.load(open(model_path + model_name + '.pickle', 'rb'))\n",
    "    if pca:\n",
    "        pca_vectorizer = pickle.load(open(model_path + 'pca_vectorizer.pickle', \"rb\"))\n",
    "    else:\n",
    "        pca_vectorizer = None\n",
    "    tfidf_vectorizer = pickle.load(open(model_path + 'tfidf_vectorizer.pickle', \"rb\"))\n",
    "    \n",
    "    return ML_model, pca_vectorizer, tfidf_vectorizer"
   ],
   "id": "264fe0edc970938e"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-03T20:11:13.445736900Z",
     "start_time": "2023-12-03T20:11:13.418737300Z"
    }
   },
   "outputs": [],
   "source": [
    "# load dataset for testing\n",
    "fold_parent = './data/dronology_five_folds/'\n",
    "\n",
    "sub_folders = []\n",
    "for folder in os.listdir(fold_parent):\n",
    "    if 'fold' in folder: \n",
    "        sub_folders.append(os.path.join(fold_parent, folder))\n",
    "        "
   ],
   "id": "feee676eb03b0812"
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-03T20:54:12.703517300Z",
     "start_time": "2023-12-03T20:54:12.675489Z"
    }
   },
   "outputs": [],
   "source": [
    "# replace the value of 'model_name' with desired tradional ML model's name to get results for the model\n",
    "# to trigger more traditional ML models check the names in: model/ML_models. examples, DT, SVM, pLR etc. \n",
    "# put 'p' infront of the model name to couple our pre-processing pipeline\n",
    "model_name = 'SVM'\n",
    "PCA = True\n",
    "map_labels = {0: 'information', 1: 'requirement'}"
   ],
   "id": "66b55c55890f84a6"
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-03T20:54:14.115497400Z",
     "start_time": "2023-12-03T20:54:14.000496800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results for dataset fold number : 1 on model : SVM\n",
      "\n",
      "               precision    recall  f1-score    support\n",
      "information    0.823529  1.000000  0.903226  56.000000\n",
      "requirement    1.000000  0.400000  0.571429  20.000000\n",
      "accuracy       0.842105  0.842105  0.842105   0.842105\n",
      "macro avg      0.911765  0.700000  0.737327  76.000000\n",
      "weighted avg   0.869969  0.842105  0.815911  76.000000\n",
      "--------------------------------------\n",
      "\n",
      "Results for dataset fold number : 2 on model : SVM\n",
      "\n",
      "               precision    recall  f1-score    support\n",
      "information    0.787879  0.928571  0.852459  56.000000\n",
      "requirement    0.600000  0.300000  0.400000  20.000000\n",
      "accuracy       0.763158  0.763158  0.763158   0.763158\n",
      "macro avg      0.693939  0.614286  0.626230  76.000000\n",
      "weighted avg   0.738437  0.763158  0.733391  76.000000\n",
      "--------------------------------------\n",
      "\n",
      "Results for dataset fold number : 3 on model : SVM\n",
      "\n",
      "               precision    recall  f1-score  support\n",
      "information    0.768116  0.963636  0.854839    55.00\n",
      "requirement    0.666667  0.200000  0.307692    20.00\n",
      "accuracy       0.760000  0.760000  0.760000     0.76\n",
      "macro avg      0.717391  0.581818  0.581266    75.00\n",
      "weighted avg   0.741063  0.760000  0.708933    75.00\n",
      "--------------------------------------\n",
      "\n",
      "Results for dataset fold number : 4 on model : SVM\n",
      "\n",
      "               precision    recall  f1-score    support\n",
      "information    0.805970  0.981818  0.885246  55.000000\n",
      "requirement    0.875000  0.350000  0.500000  20.000000\n",
      "accuracy       0.813333  0.813333  0.813333   0.813333\n",
      "macro avg      0.840485  0.665909  0.692623  75.000000\n",
      "weighted avg   0.824378  0.813333  0.782514  75.000000\n",
      "--------------------------------------\n",
      "\n",
      "Results for dataset fold number : 5 on model : SVM\n",
      "\n",
      "               precision    recall  f1-score    support\n",
      "information    0.776119  0.928571  0.845528  56.000000\n",
      "requirement    0.500000  0.210526  0.296296  19.000000\n",
      "accuracy       0.746667  0.746667  0.746667   0.746667\n",
      "macro avg      0.638060  0.569549  0.570912  75.000000\n",
      "weighted avg   0.706169  0.746667  0.706390  75.000000\n",
      "--------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ahmii\\AppData\\Local\\Temp\\ipykernel_13164\\130388930.py:14: DeprecationWarning: Please use `csr_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.csr` namespace is deprecated.\n",
      "  tfidf_vectorizer = pickle.load(open(model_path + 'tfidf_vectorizer.pickle', \"rb\"))\n",
      "C:\\Users\\ahmii\\AppData\\Local\\Temp\\ipykernel_13164\\130388930.py:14: DeprecationWarning: Please use `csr_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.csr` namespace is deprecated.\n",
      "  tfidf_vectorizer = pickle.load(open(model_path + 'tfidf_vectorizer.pickle', \"rb\"))\n",
      "C:\\Users\\ahmii\\AppData\\Local\\Temp\\ipykernel_13164\\130388930.py:14: DeprecationWarning: Please use `csr_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.csr` namespace is deprecated.\n",
      "  tfidf_vectorizer = pickle.load(open(model_path + 'tfidf_vectorizer.pickle', \"rb\"))\n",
      "C:\\Users\\ahmii\\AppData\\Local\\Temp\\ipykernel_13164\\130388930.py:14: DeprecationWarning: Please use `csr_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.csr` namespace is deprecated.\n",
      "  tfidf_vectorizer = pickle.load(open(model_path + 'tfidf_vectorizer.pickle', \"rb\"))\n",
      "C:\\Users\\ahmii\\AppData\\Local\\Temp\\ipykernel_13164\\130388930.py:14: DeprecationWarning: Please use `csr_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.csr` namespace is deprecated.\n",
      "  tfidf_vectorizer = pickle.load(open(model_path + 'tfidf_vectorizer.pickle', \"rb\"))\n"
     ]
    }
   ],
   "source": [
    "# load test data & make prediction\n",
    "\n",
    "ml_results = []\n",
    "avg_accuracy = []\n",
    "fold_count = 1\n",
    "\n",
    "for subs in sorted(sub_folders):\n",
    "    test_path = subs + '/test_' + 'fold_' + str(fold_count) + '.csv'\n",
    "    \n",
    "    df_test = pd.read_csv(test_path)\n",
    "    df_test['STR.REQ'] = df_test['STR.REQ'].str.lower()\n",
    "    X_test = df_test['STR.REQ']\n",
    "    y_test = df_test['class']\n",
    "    \n",
    "    model_path = 'D:\\Github Projects -Parth\\REFSQ2023-ReqORNot\\models\\ML_models\\\\' + model_name + '\\\\fold_' + str(fold_count) + '\\\\'\n",
    "    ML_model, pca_vectorizer, tfidf_vectorizer = load_ML_model_files(model_name, model_path, PCA)\n",
    "\n",
    "    tfidf_vecs = tfidf_vectorizer.transform(X_test)\n",
    "    normalized_tfidf = normalize(tfidf_vecs)\n",
    "\n",
    "    test_vecs = pca_vectorizer.transform(normalized_tfidf.toarray())\n",
    "    predicted_labels = ML_model.predict(test_vecs)\n",
    "    \n",
    "    evaluation_results = classification_report(y_test.tolist(), predicted_labels.tolist(), \n",
    "                                               target_names=list(map_labels.values()), \n",
    "                                               output_dict=True)\n",
    "    \n",
    "    avg_accuracy.append(get_accuracy(y_test.tolist(), predicted_labels.tolist()))\n",
    "    \n",
    "    report_df = pd.DataFrame(evaluation_results).transpose()\n",
    "    ml_results.append(report_df)\n",
    "    \n",
    "    print('\\nResults for dataset fold number :',fold_count, 'on model :', model_name)\n",
    "    print('\\n',report_df)\n",
    "    print('--------------------------------------')\n",
    "    \n",
    "    fold_count += 1\n",
    "\n"
   ],
   "id": "c2b294b261571cb"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-03T20:11:27.924138600Z",
     "start_time": "2023-12-03T20:11:27.885242600Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "              Precision  Recall  F1_score\n5-folds                                  \nweighted_avg       0.78    0.79      0.75\nmacro_avg          0.76    0.63      0.64\naccuracy_avg       0.78    0.78      0.78",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1_score</th>\n    </tr>\n    <tr>\n      <th>5-folds</th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>weighted_avg</th>\n      <td>0.78</td>\n      <td>0.79</td>\n      <td>0.75</td>\n    </tr>\n    <tr>\n      <th>macro_avg</th>\n      <td>0.76</td>\n      <td>0.63</td>\n      <td>0.64</td>\n    </tr>\n    <tr>\n      <th>accuracy_avg</th>\n      <td>0.78</td>\n      <td>0.78</td>\n      <td>0.78</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Average results of ML pipeline\n",
    "\n",
    "avg_acc_score = round(np.mean(avg_accuracy), 2)\n",
    "weighted_avg, macro_avg = get_avg_report(ml_results, folds=5)\n",
    "\n",
    "avg_scores = list([weighted_avg, macro_avg, (avg_acc_score, avg_acc_score, avg_acc_score)])\n",
    "\n",
    "final_df = pd.DataFrame([x for x in avg_scores], columns=(['Precision', 'Recall', 'F1_score']),\n",
    "                      index=['weighted_avg','macro_avg', 'accuracy_avg'])\n",
    "\n",
    "final_df.rename_axis('5-folds')"
   ],
   "id": "367c253d9cca5718"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT Family Pipeline"
   ],
   "id": "2c687c9af0de6114"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-03T20:11:33.414112900Z",
     "start_time": "2023-12-03T20:11:33.391113Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_tokenizer(model_name):\n",
    "    \n",
    "    \"\"\"\n",
    "    loads and returns the relevant tokenizer for passed parameter BERT model name\n",
    "    \n",
    "    \"\"\"\n",
    "    if model_name in ('BERT_base_uncased', \n",
    "                      'pBERT_base_uncased'):\n",
    "        tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\",\n",
    "                                                  do_lower_case=True)\n",
    "                \n",
    "    elif model_name in ('BERT_base_cased',\n",
    "                        'pBERT_base_cased'):\n",
    "        tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "    \n",
    "    elif model_name in ('pXLNet_base', \n",
    "                        'XLNet_base'):\n",
    "        tokenizer = XLNetTokenizer.from_pretrained(\"xlnet-base-cased\")\n",
    "    \n",
    "    elif model_name in ('SciBERT_uncased', \n",
    "                        'pSciBERT_uncased'):\n",
    "        tokenizer = BertTokenizer.from_pretrained('allenai/scibert_scivocab_uncased', \n",
    "                                                  do_lower_case=True)\n",
    "    \n",
    "    elif model_name in ('pRoBERTa_base', \n",
    "                        'RoBERTa_base'):\n",
    "        tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "\n",
    "    elif model_name in ('DisBERT_base_cased', \n",
    "                        'pDisBERT_base_cased'):\n",
    "        tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-cased\")\n",
    "    \n",
    "    elif model_name in ('DisBERT_base_uncased', \n",
    "                        'pDisBERT_base_uncased'):\n",
    "        tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "    else:\n",
    "        #'pXRBERT_base', 'XRBERT_base'\n",
    "        tokenizer = XLMRobertaTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
    "    \n",
    "    return tokenizer\n",
    "        "
   ],
   "id": "72abd69d8243b9d6"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-03T20:11:35.162087900Z",
     "start_time": "2023-12-03T20:11:35.145077700Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_BERT_model(model_name, model_path):\n",
    "    \"\"\"\n",
    "    loads and returns the BERT model based on the model name and path parameters\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    if model_name in ('BERT_base_uncased', 'pBERT_base_cased',\n",
    "                      'pBERT_base_uncased', 'BERT_base_cased',\n",
    "                      'SciBERT_uncased', 'pSciBERT_uncased'\n",
    "                     ):\n",
    "        model = BertForSequenceClassification.from_pretrained(model_path)                \n",
    "    elif model_name in ('pXLNet_base', \n",
    "                        'XLNet_base'\n",
    "                       ):\n",
    "        model = XLNetForSequenceClassification.from_pretrained(model_path)\n",
    "    \n",
    "    elif model_name in ('pRoBERTa_base', \n",
    "                        'RoBERTa_base'\n",
    "                       ):\n",
    "        model = RobertaForSequenceClassification.from_pretrained(model_path)\n",
    "\n",
    "    elif model_name in ('DisBERT_base_cased', 'DisBERT_base_uncased',\n",
    "                        'pDisBERT_base_cased', 'pDisBERT_base_uncased'\n",
    "                       ):\n",
    "        model = DistilBertForSequenceClassification.from_pretrained(model_path)    \n",
    "    \n",
    "    else:\n",
    "        #'pXRBERT_base', 'XRBERT_base'\n",
    "        model = XLMRobertaForSequenceClassification.from_pretrained(model_path)\n",
    "    \n",
    "    return model"
   ],
   "id": "f1a9b5038c200d7e"
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-03T20:32:30.577313Z",
     "start_time": "2023-12-03T20:32:28.471295300Z"
    }
   },
   "outputs": [],
   "source": [
    "# replace the value of 'model_name' with BERT model's name to get results for the model\n",
    "# to trigger more BERT models check the names in: model/BERT_family. examples, BERT_base_cased etc. \n",
    "# put 'p' infront of the model name to couple our pre-processing pipeline\n",
    "\n",
    "map_labels = {0: 'information', 1: 'requirement'}\n",
    "\n",
    "prefix = './models/BERT_family/'\n",
    "model_name = 'DisBERT_base_uncased'\n",
    "\n",
    "fold_parent = './data/dronology_five_folds/'\n",
    "\n",
    "sub_folders = []\n",
    "for folder in os.listdir(fold_parent):\n",
    "    if 'fold' in folder: \n",
    "        sub_folders.append(os.path.join(fold_parent, folder))\n",
    "\n",
    "tokenizer = load_tokenizer(model_name)\n",
    "MAX_SEQ_LENGTH = 128"
   ],
   "id": "102e37b4dbb95ed0"
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-03T20:32:30.600310400Z",
     "start_time": "2023-12-03T20:32:30.577313Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "PreTrainedTokenizer(name_or_path='distilbert-base-uncased', vocab_size=30522, model_max_len=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ],
   "id": "4f4ccdb60afe2f08"
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-03T20:32:56.792079900Z",
     "start_time": "2023-12-03T20:32:31.507318100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/dronology_five_folds/fold_1/test_fold_1.csv\n",
      "./models/BERT_family/DisBERT_base_uncased/fold_1/*\n",
      "\n",
      "Results for dataset fold number : 1 on model : DisBERT_base_uncased\n",
      "\n",
      "               precision    recall  f1-score    support\n",
      "information    0.981132  0.928571  0.954128  56.000000\n",
      "requirement    0.826087  0.950000  0.883721  20.000000\n",
      "accuracy       0.934211  0.934211  0.934211   0.934211\n",
      "macro avg      0.903610  0.939286  0.918925  76.000000\n",
      "weighted avg   0.940331  0.934211  0.935600  76.000000\n",
      "--------------------------------------\n",
      "./data/dronology_five_folds/fold_2/test_fold_2.csv\n",
      "./models/BERT_family/DisBERT_base_uncased/fold_2/*\n",
      "\n",
      "Results for dataset fold number : 2 on model : DisBERT_base_uncased\n",
      "\n",
      "               precision    recall  f1-score    support\n",
      "information    0.833333  0.892857  0.862069  56.000000\n",
      "requirement    0.625000  0.500000  0.555556  20.000000\n",
      "accuracy       0.789474  0.789474  0.789474   0.789474\n",
      "macro avg      0.729167  0.696429  0.708812  76.000000\n",
      "weighted avg   0.778509  0.789474  0.781408  76.000000\n",
      "--------------------------------------\n",
      "./data/dronology_five_folds/fold_3/test_fold_3.csv\n",
      "./models/BERT_family/DisBERT_base_uncased/fold_3/*\n",
      "\n",
      "Results for dataset fold number : 3 on model : DisBERT_base_uncased\n",
      "\n",
      "               precision    recall  f1-score  support\n",
      "information    0.942308  0.890909  0.915888    55.00\n",
      "requirement    0.739130  0.850000  0.790698    20.00\n",
      "accuracy       0.880000  0.880000  0.880000     0.88\n",
      "macro avg      0.840719  0.870455  0.853293    75.00\n",
      "weighted avg   0.888127  0.880000  0.882504    75.00\n",
      "--------------------------------------\n",
      "./data/dronology_five_folds/fold_4/test_fold_4.csv\n",
      "./models/BERT_family/DisBERT_base_uncased/fold_4/*\n",
      "\n",
      "Results for dataset fold number : 4 on model : DisBERT_base_uncased\n",
      "\n",
      "               precision    recall  f1-score    support\n",
      "information    0.944444  0.927273  0.935780  55.000000\n",
      "requirement    0.809524  0.850000  0.829268  20.000000\n",
      "accuracy       0.906667  0.906667  0.906667   0.906667\n",
      "macro avg      0.876984  0.888636  0.882524  75.000000\n",
      "weighted avg   0.908466  0.906667  0.907377  75.000000\n",
      "--------------------------------------\n",
      "./data/dronology_five_folds/fold_5/test_fold_5.csv\n",
      "./models/BERT_family/DisBERT_base_uncased/fold_5/*\n",
      "\n",
      "Results for dataset fold number : 5 on model : DisBERT_base_uncased\n",
      "\n",
      "               precision    recall  f1-score    support\n",
      "information    0.809524  0.910714  0.857143  56.000000\n",
      "requirement    0.583333  0.368421  0.451613  19.000000\n",
      "accuracy       0.773333  0.773333  0.773333   0.773333\n",
      "macro avg      0.696429  0.639568  0.654378  75.000000\n",
      "weighted avg   0.752222  0.773333  0.754409  75.000000\n",
      "--------------------------------------\n"
     ]
    }
   ],
   "source": [
    "fold_count = 1\n",
    "results = []\n",
    "avg_accuracy = []\n",
    "for subs in sorted(sub_folders):\n",
    "    test_path = subs + '/test_' + 'fold_' + str(fold_count) + '.csv'\n",
    "    print(test_path)\n",
    "    df_test = pd.read_csv(test_path)\n",
    "    selected_test = df_test[['STR.REQ','class']]\n",
    "\n",
    "    test_sequences = selected_test['STR.REQ'].tolist()\n",
    "\n",
    "    test_encodings = tokenizer(test_sequences, truncation=True, \n",
    "                               padding=True, \n",
    "                               max_length=MAX_SEQ_LENGTH, \n",
    "                               return_tensors=\"pt\")\n",
    "    # load model\n",
    "    # model_path = glob.glob(prefix + model_name + '/fold_' + str(fold_count) + '/*')[0]\n",
    "    # bert_model = load_BERT_model(model_name, model_path)\n",
    "    # \n",
    "    # with torch.no_grad():\n",
    "    #     logits = bert_model(**test_encodings).logits\n",
    "    # \n",
    "    # predictions = np.argmax(logits, axis=1)\n",
    "    # evaluation_results = classification_report(selected_test['class'].tolist(), \n",
    "    #                                            predictions.tolist(), \n",
    "    #                                            target_names=list(map_labels.values()), \n",
    "    #                                            output_dict=True)\n",
    "    # \n",
    "    # avg_accuracy.append(get_accuracy(selected_test['class'].tolist(), \n",
    "    #                                  predictions.tolist()))\n",
    "    # \n",
    "    # report_df = pd.DataFrame(evaluation_results).transpose()\n",
    "    # results.append(report_df)\n",
    "    # \n",
    "    # print('\\nResults for dataset fold number :',fold_count, 'on model :', model_name)\n",
    "    # print('\\n',report_df)\n",
    "    # print('--------------------------------------')\n",
    "    # \n",
    "    # fold_count += 1\n",
    "    \n",
    "    model_paths = glob.glob(prefix + model_name + '/fold_' + str(fold_count) + '/*')\n",
    "    print(prefix + model_name + '/fold_' + str(fold_count)+ '/*')\n",
    "    if model_paths:\n",
    "        model_path = model_paths[0]\n",
    "        \n",
    "        bert_model = load_BERT_model(model_name, model_path)\n",
    "        \n",
    "        # Rest of your code for model evaluation\n",
    "        with torch.no_grad():\n",
    "            logits = bert_model(**test_encodings).logits\n",
    "    \n",
    "        predictions = np.argmax(logits, axis=1)\n",
    "        evaluation_results = classification_report(selected_test['class'].tolist(), \n",
    "                                                   predictions.tolist(), \n",
    "                                                   target_names=list(map_labels.values()), \n",
    "                                                   output_dict=True)\n",
    "    \n",
    "        avg_accuracy.append(get_accuracy(selected_test['class'].tolist(), \n",
    "                                         predictions.tolist()))\n",
    "    \n",
    "        report_df = pd.DataFrame(evaluation_results).transpose()\n",
    "        results.append(report_df)\n",
    "    \n",
    "        print('\\nResults for dataset fold number :',fold_count, 'on model :', model_name)\n",
    "        print('\\n',report_df)\n",
    "        print('--------------------------------------')\n",
    "        \n",
    "    else:\n",
    "        print(f\"No model files found for fold {fold_count} and model {model_name}.\")    \n",
    "    fold_count+=1"
   ],
   "id": "78037a71c8429689"
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-03T20:33:02.979201300Z",
     "start_time": "2023-12-03T20:33:02.963202500Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "              Precision  Recall  F1_score\n5-folds                                  \nweighted_avg       0.85    0.86      0.85\nmacro_avg          0.81    0.81      0.80\naccuracy_avg       0.86    0.86      0.86",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1_score</th>\n    </tr>\n    <tr>\n      <th>5-folds</th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>weighted_avg</th>\n      <td>0.85</td>\n      <td>0.86</td>\n      <td>0.85</td>\n    </tr>\n    <tr>\n      <th>macro_avg</th>\n      <td>0.81</td>\n      <td>0.81</td>\n      <td>0.80</td>\n    </tr>\n    <tr>\n      <th>accuracy_avg</th>\n      <td>0.86</td>\n      <td>0.86</td>\n      <td>0.86</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Average results of BERT model\n",
    "\n",
    "avg_acc_score = round(np.mean(avg_accuracy), 2)\n",
    "weighted_avg, macro_avg = get_avg_report(results, folds=5)\n",
    "\n",
    "avg_scores = list([weighted_avg, macro_avg, (avg_acc_score, avg_acc_score, \n",
    "                                             avg_acc_score)])\n",
    "\n",
    "final_df = pd.DataFrame([x for x in avg_scores], \n",
    "                        columns=(['Precision', 'Recall', 'F1_score']),\n",
    "                        index=['weighted_avg','macro_avg', 'accuracy_avg'])\n",
    "\n",
    "final_df.rename_axis('5-folds')"
   ],
   "id": "c480b09f61165f85"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Pipeline"
   ],
   "id": "a9484683e38a5093"
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-03T20:49:10.740132900Z",
     "start_time": "2023-12-03T20:49:10.668131400Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_random_label(ranges):\n",
    "    \"\"\"\n",
    "    returns the random label from the defined ranges of the labels\n",
    "    \"\"\"\n",
    "    temp=random.randint(1, ranges[-1][-1])\n",
    "    \n",
    "    for r in ranges:\n",
    "        if(temp>r[1] and temp<=r[-1]):\n",
    "            return r[0]\n",
    "    return None\n",
    "\n",
    "def get_ranges(df):\n",
    "    \"\"\"\n",
    "    predicts the random labels on the given test dataset\n",
    "    \n",
    "    \"\"\"\n",
    "    csum = 0\n",
    "    ranges = []\n",
    "    total_tr = len(df)\n",
    "\n",
    "    for k, v in df['class'].value_counts().to_dict().items():\n",
    "\n",
    "        csum_old = csum\n",
    "        csum += round((v/total_tr) * 100,0)\n",
    "        #print (k,\"from\", csum_old, \"to\",csum)\n",
    "        ranges.append([k, csum_old, csum])\n",
    "    \n",
    "    r_out = []\n",
    "    for row in test_df.iterrows():\n",
    "        r3labels = []\n",
    "\n",
    "        while len(r3labels)!=1:\n",
    "            rl = get_random_label(ranges)\n",
    "            if not rl in r3labels:\n",
    "                r3labels.append(rl)\n",
    "\n",
    "        r_out.append([row[1]['issueid'], row[1]['class'], r3labels])\n",
    "\n",
    "    return ranges, r_out"
   ],
   "id": "97ccbf9bfbfecaa9"
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-03T20:49:10.765132100Z",
     "start_time": "2023-12-03T20:49:10.683131800Z"
    }
   },
   "outputs": [],
   "source": [
    "# load dataset\n",
    "fold_parent = './data/dronology_five_folds/'\n",
    "\n",
    "sub_folders = []\n",
    "for folder in os.listdir(fold_parent):\n",
    "    if 'fold' in folder: \n",
    "        sub_folders.append(os.path.join(fold_parent, folder)) "
   ],
   "id": "cecc2fc912d86cc3"
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-03T20:49:10.834171700Z",
     "start_time": "2023-12-03T20:49:10.700133400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results for fold number : 1\n",
      "\n",
      "               precision    recall  f1-score    support\n",
      "information    0.760000  0.678571  0.716981  56.000000\n",
      "requirement    0.307692  0.400000  0.347826  20.000000\n",
      "accuracy       0.605263  0.605263  0.605263   0.605263\n",
      "macro avg      0.533846  0.539286  0.532404  76.000000\n",
      "weighted avg   0.640972  0.605263  0.619835  76.000000\n",
      "--------------------------------------\n",
      "\n",
      "Results for fold number : 2\n",
      "\n",
      "               precision    recall  f1-score    support\n",
      "information    0.679245  0.642857  0.660550  56.000000\n",
      "requirement    0.130435  0.150000  0.139535  20.000000\n",
      "accuracy       0.513158  0.513158  0.513158   0.513158\n",
      "macro avg      0.404840  0.396429  0.400043  76.000000\n",
      "weighted avg   0.534821  0.513158  0.523441  76.000000\n",
      "--------------------------------------\n",
      "\n",
      "Results for fold number : 3\n",
      "\n",
      "               precision    recall  f1-score    support\n",
      "information    0.740000  0.672727  0.704762  55.000000\n",
      "requirement    0.280000  0.350000  0.311111  20.000000\n",
      "accuracy       0.586667  0.586667  0.586667   0.586667\n",
      "macro avg      0.510000  0.511364  0.507937  75.000000\n",
      "weighted avg   0.617333  0.586667  0.599788  75.000000\n",
      "--------------------------------------\n",
      "\n",
      "Results for fold number : 4\n",
      "\n",
      "               precision    recall  f1-score    support\n",
      "information    0.764706  0.709091  0.735849  55.000000\n",
      "requirement    0.333333  0.400000  0.363636  20.000000\n",
      "accuracy       0.626667  0.626667  0.626667   0.626667\n",
      "macro avg      0.549020  0.554545  0.549743  75.000000\n",
      "weighted avg   0.649673  0.626667  0.636592  75.000000\n",
      "--------------------------------------\n",
      "\n",
      "Results for fold number : 5\n",
      "\n",
      "               precision    recall  f1-score    support\n",
      "information    0.771930  0.785714  0.778761  56.000000\n",
      "requirement    0.333333  0.315789  0.324324  19.000000\n",
      "accuracy       0.666667  0.666667  0.666667   0.666667\n",
      "macro avg      0.552632  0.550752  0.551543  75.000000\n",
      "weighted avg   0.660819  0.666667  0.663637  75.000000\n",
      "--------------------------------------\n"
     ]
    }
   ],
   "source": [
    "fold_count = 1\n",
    "for subs in sorted(sub_folders):\n",
    "    \n",
    "    test_path = subs + '/test_' + 'fold_' + str(fold_count) + '.csv'\n",
    "    test_df = pd.read_csv(test_path)\n",
    "    ranges, r_out = get_ranges(test_df)\n",
    "    \n",
    "    random_out = pd.DataFrame()\n",
    "    random_out['issueid'] = [i[0] for i in r_out]\n",
    "    random_out['class'] = [i[1] for i in r_out]\n",
    "    random_out['top_label'] = [i[2][0] for i in r_out]\n",
    "    evaluation_results = classification_report(random_out['class'], random_out['top_label'], \n",
    "                                               target_names=list(map_labels.values()), \n",
    "                                               output_dict=True)\n",
    "    \n",
    "    report_df = pd.DataFrame(evaluation_results).transpose()\n",
    "    print('\\nResults for fold number :',fold_count)\n",
    "    print('\\n',report_df)\n",
    "    print('--------------------------------------')\n",
    "    \n",
    "    fold_count += 1"
   ],
   "id": "fc855900a64b9170"
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-03T20:49:10.835133400Z",
     "start_time": "2023-12-03T20:49:10.791133700Z"
    }
   },
   "id": "7fd6c0e9f5ec122a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "74905b89b0793fd2"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
